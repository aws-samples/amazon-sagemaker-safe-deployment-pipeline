{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe MLOps Deployment Pipeline\n",
    "\n",
    "Julian Bright, Machine Learning Specialist @ Amazon Web Services\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook you will step through an MLOps pipeline to build, train, deploy and monitor an XGBoost regression model for predicting the expected taxi fare using the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/). This safe pipeline features a [canary deployment](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/canary-deployment.html) strategy with rollback on error. You will learn how to trigger and monitor the pipeline, inspect the training workflow, use model monitor to set up alerts, and create a canary deployment.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Note: This notebook assumes prior familiarity with the basics training ML models on Amazon SageMaker. Data preparation and visualization, although present, will be kept to a minimum. If you are not familiar with the basic concepts and features of SageMaker, we recommend reading the <a href=https://docs.aws.amazon.com/sagemaker/>SageMaker documentation</a> and completing the workshops and samples in <a href=https://github.com/aws/amazon-sagemaker-examples>AWS SageMaker Examples GitHub</a> and <a href=https://github.com/aws-samples?q=sagemaker&type=&language=>AWS Samples GitHub</a>. \n",
    "</div>\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The architecture diagram below shows the entire MLOps pipeline at a high level.\n",
    "\n",
    "Use the CloudFormation template provided in this repository (`pipeline.yml`) to build the demo in your own AWS account. If you are currently viewing this notebook from SageMaker in your AWS account, then you have already completed this step. CloudFormation deploys several resources:\n",
    "   \n",
    "1. A customer-managed encryption key in in Amazon KMS for encrypting data and artifacts.\n",
    "1. A secret in Amazon Secrets Manager to securely store your GitHub Access Token.\n",
    "1. Several AWS IAM roles so CloudFormation, SageMaker, and other AWS services can perform actions in your AWS account, following the principle of [least privilege](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege).\n",
    "1. A messaging service in Amazon SNS to notify you when certain steps of the pipeline have been completed (signing up for these notifications is optional).\n",
    "1. Two Amazon CloudWatch event rules: one which schedules the pipeline to run every month, and one which triggers the pipeline to run when SageMaker Model Monitor detects certain metrics.\n",
    "1. An Amazon SageMaker Jupyter notebook with this workshop content pre-loaded.\n",
    "1. An S3 bucket for storing model artifacts.\n",
    "1. An AWS CodePipeline instance with several pre-defined stages. \n",
    "\n",
    "Take a moment to look at all of these resources now deployed in your account. \n",
    "\n",
    "![MLOps pipeline architecture](../docs/mlops-architecture.png)\n",
    "\n",
    "In this notebook, you will work through the CodePipeline instance created by the CloudFormation template. It has several stages:\n",
    "\n",
    "1. **Source** - The pipeline is already configured with two sources. If you upload a new dataset to a specific location in the S3 data bucket, this will trigger the pipeline to run. Similarly, if you commit new code to your fork of the GitHub repository for this demo, this will trigger the pipeline to run. \n",
    "1. **Build** - In this stage, CodeBuild will run `model/buildspec.yml` and `model/run.py` to generate a Step Functions template and CloudFormation templates based on the data sets and hyperparameters specified for this pipeline run. You will take a closer look at these files later in this notebook. CloudFormation is used to set up the Step Functions workflow based on the templates. Note that this stage does not run the Step Functions workflow - it only sets up the resources which are needed in next stages.\n",
    "1. **Train** The Step Functions workflow set up in the previous stage is run in this stage. It creates a baseline for the model monitor using a SageMaker processing job, and trains an XGBoost model on the taxi ride dataset using a SageMaker training job.\n",
    "1. **Deploy Dev** In this stage, CloudFormation uses the model artifacts created in the previous stage (which includes a version of `assets/deploy-model-dev.yml`) to set up a SageMaker API endpoint. This will allow you to run tests on the model and decide if the model is of sufficient quality to deploy into production. \n",
    "1. **Deploy Production** The final stage of the pipeline is the only stage which does not run automatically as soon as the previous stage is complete. It waits for a user to manually approve the model which was previously deployed to dev. As soon as the model is approved, this stage uses CloudFormation (through `assets/deploy-model-prod.yml`) to deploy the model into a production application. It creates a new SageMaker endpoint with autoscaling and data capture enabled. It creates a model monitoring schedule and sets CloudWatch alarms for certain metrics. And it sets up an AWS CodeDeploy instance which deploys a set of AWS Lambda functions and an Amazon API Gateway to sit in front of the SageMaker endpoint. This stage can make use of canary deployment to safely switch from an old model to a new model. \n",
    "\n",
    "### Content\n",
    "\n",
    "This notebook has the following structure:\n",
    "\n",
    "1. [Data Preparation](#Data-Preparation)\n",
    "2. [Start Model Build](#Start-Model-Build)\n",
    "3. [Inspect Training Job](#Inspect-Training-Job)\n",
    "4. [Test Dev Deployment](#Test-Dev-Deployment)\n",
    "5. [Approve Deployment to Production](#Approve-Deployment-to-Production)\n",
    "6. [Test Production Deployment](#Test-Production-Deployment)\n",
    "7. [Inspect Model Monitor](#Inspect-Model-Monitor)\n",
    "8. [CloudWatch Monitoring](#CloudWatch-Monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the latest sagemaker and boto3 SDKs\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" tqdm\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0rc1\"\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation <a class=\"anchor\" id=\"Data-Prep\"></a>\n",
    "\n",
    "If you navigate to the [CodePipeline](https://console.aws.amazon.com/codesuite/codepipeline/pipelines?region=us-east-1&pipelines-meta=%7B%22f%22%3A%7B%22text%22%3A%22%22%7D%2C%22s%22%3A%7B%22property%22%3A%22updated%22%2C%22direction%22%3A-1%7D%2C%22n%22%3A10%2C%22i%22%3A0%7D) instance created for this workshop, you will notice that it starts in a `Failed` state. This happens because the dataset, which is one of the sources that can trigger the pipeline, has not yet been uploaded to the S3 bucket. In this section of the notebook, you will download the publicly available New York Taxi dataset and upload it to the location expected by the pipeline.\n",
    "\n",
    "![Failed code pipeline](../docs/pipeline_failed.png)\n",
    "\n",
    "First, download a sample of the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) to this notebook instance. This dataset contains information on trips taken by taxis and for-hire vehicles in New York City, including pick-up and drop-off times and locations, fares, distance traveled, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the dataset into a pandas data frame, taking care to parse the dates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parse_dates= ['lpep_dropoff_datetime', 'lpep_pickup_datetime']\n",
    "trip_df = pd.read_csv('nyc-tlc.csv', parse_dates=parse_dates)\n",
    "\n",
    "trip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "\n",
    "Instead of the raw date and time features for pick-up and drop-off, let's use these features to calculate the total time of the trip in minutes, which will be easier to work with for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df['duration_minutes'] = (trip_df['lpep_dropoff_datetime'] - trip_df['lpep_pickup_datetime']).dt.seconds/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of columns we don't need, so let's select a sample of columns for our machine learning model. Keep only `total_amount` (fare), `duration_minutes`, `passenger_count`, and `trip_distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['total_amount', 'duration_minutes', 'passenger_count', 'trip_distance']\n",
    "data_df = trip_df[cols]\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some quick statistics for the dataset to understand the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows some clear outliers, e.g. -400 or 2626 as fare, or 0 passengers. There are many intelligent methods for identifying and removing outliers, but data cleaning is not the focus of this notebook, so just remove the outliers by setting some min and max values which seem more reasonable. Removing the outliers results in a final dataset of 754,671 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[(data_df.total_amount > 0) & (data_df.total_amount < 200) & \n",
    "                  (data_df.duration_minutes > 0) & (data_df.duration_minutes < 120) & \n",
    "                  (data_df.trip_distance > 0) & (data_df.trip_distance < 121) & \n",
    "                  (data_df.passenger_count > 0)].dropna()\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Since this notebook will build a regression model for the taxi data, it's a good idea to check if there is any correlation between the variables in our data. Use scatter plots on a sample of the data to compare trip distance with duration in minutes, and total amount (fare) with duration in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sample_df = data_df.sample(1000)\n",
    "sns.scatterplot(data=sample_df, x='duration_minutes', y='trip_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample_df, x='duration_minutes', y='total_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scatter plots look fine and show at least some correlation between our variables. \n",
    "\n",
    "### Data splitting and saving\n",
    "\n",
    "We are now ready to split the dataset into train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\n",
    "\n",
    "# Set the index for our test dataframe\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('Size of\\n train: {},\\n val: {},\\n test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train, validation, and test files as CSV locally on this notebook instance. Notice that you save the train file twice - once as the training data file and once as the baseline data file. The baseline data file will be used by [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) to detect data drift. Data drift occurs when the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, which means the model begins to lose accuracy in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['total_amount', 'duration_minutes','passenger_count','trip_distance']\n",
    "train_df.to_csv('train.csv', index=False, header=False)\n",
    "val_df.to_csv('validation.csv', index=False, header=False)\n",
    "test_df.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Save test and baseline with headers\n",
    "train_df.to_csv('baseline.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload these CSV files to your default SageMaker S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Specify data previx version\n",
    "prefix = 'nyc-tlc/v1'\n",
    "\n",
    "s3_train_uri = session.upload_data('train.csv', bucket, prefix + '/data/training')\n",
    "s3_val_uri = session.upload_data('validation.csv', bucket, prefix + '/data/validation')\n",
    "s3_test_uri = session.upload_data('test.csv', bucket, prefix + '/data/test')\n",
    "s3_baseline_uri = session.upload_data('baseline.csv', bucket, prefix + '/data/baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the datasets which you have prepared and saved in this section to trigger the pipeline to train and deploy a model in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Model Build\n",
    "\n",
    "In this section, you will start a model build and deployment pipeline by packaging up the datasets you prepared in the previous section and uploading these to the S3 source location which triggers the [CodePipeline](https://console.aws.amazon.com/codesuite/codepipeline/pipelines?region=us-east-1&pipelines-meta=%7B%22f%22%3A%7B%22text%22%3A%22%22%7D%2C%22s%22%3A%7B%22property%22%3A%22updated%22%2C%22direction%22%3A-1%7D%2C%22n%22%3A10%2C%22i%22%3A0%7D) instance created for this workshop. \n",
    "\n",
    "First, import some libraries and load some environment variables which you will need. These environment variables have been set through the lifecycle configuration script attached to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\n",
    "pipeline_name = os.environ['PIPELINE_NAME']\n",
    "model_name = os.environ['MODEL_NAME']\n",
    "#workflow_pipeline_arn = \"arn:aws:codepipeline:us-east-1:002268754244:nyctaxi\"\n",
    "workflow_pipeline_arn = os.environ['WORKFLOW_PIPELINE_ARN']\n",
    "\n",
    "print('region: {}'.format(region))\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\n",
    "print('pipeline: {}'.format(pipeline_name))\n",
    "print('model name: {}'.format(model_name))\n",
    "print('workflow: {}'.format(workflow_pipeline_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the AWS CodePipeline [documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-s3.html):\n",
    "\n",
    "> When Amazon S3 is the source provider for your pipeline, you may zip your source file or files into a single .zip and upload the .zip to your source bucket. You may also upload a single unzipped file; however, downstream actions that expect a .zip file will fail.\n",
    "\n",
    "To train a model, you need multiple datasets (train, validation, and test) along with a file specifying the hyperparameters. In this example, you will create one JSON file which contains the S3 dataset locations and one JSON file which contains the hyperparameter values. Then you compress both files into a zip package to be used as input for the pipeline run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "input_data = {\n",
    "    'TrainingUri': s3_train_uri,\n",
    "    'ValidationUri': s3_val_uri,\n",
    "    'TestUri': s3_test_uri,\n",
    "    'BaselineUri': s3_baseline_uri\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_round': 50\n",
    "}\n",
    "\n",
    "data_source_key = '{}/data-source.zip'.format(pipeline_name)\n",
    "\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\n",
    "    zf.writestr('inputData.json', json.dumps(input_data))\n",
    "    zf.writestr('hyperparameters.json', json.dumps(hyperparameters))\n",
    "zip_buffer.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the zip package to your artifact S3 bucket - this action will trigger the pipeline to train and deploy a model. If you already have the pipeline open in another tab, be aware that it may take a minute before the pipeline shows that it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, store the input data object to be used within the [workflow](workflow.ipynb) notebook. The workflow notebook can be used to adjust the Step Functions workflow and training job definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Training Job\n",
    "\n",
    "In this section, you will see the build pipeline run through all the steps of setting up the build templates and creating a Step Functions workflow which runs a training job for the XGBoost model. While you wait for the training job to complete, you can use the time to explore the code which is used to set this up.\n",
    "\n",
    "If you don't already have your CodePipeline instance open in another tab, run the cell below and click on the link it provides as output to navigate to your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(region, pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Tip: You may need to wait a couple of minutes before the pipeline displays its status as 'In progress'. The page will refresh automatically.\n",
    "</div>\n",
    "\n",
    "As you watch, the pipeline will run through the source and build steps, the details of which are explained in the 'Overview' section at the start of this notebook. Wait until the pipeline has started running the train step (see screenshot) before continuing with the next cells in this notebook. It takes an average of 3 minutes to reach this step.\n",
    "\n",
    "![Failed code pipeline](../docs/train-in-progress.png)\n",
    "\n",
    "When the pipeline has started running the train step, you can click on the 'AWS Step Functions' link displayed in the CodePipeline UI (see screenshot above) to view the Step Functions workflow which is running the training job. Alternatively, you can display the Step Functions workflow directly in this notebook using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.workflow import Workflow\n",
    "workflow = Workflow.attach(workflow_pipeline_arn)\n",
    "workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple run the cell below to display the Step Functions workflow, and re-run it after a few minutes to see the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executions = workflow.list_executions()\n",
    "if not executions:\n",
    "    raise(Exception('Please wait.  Training not started'))\n",
    "    \n",
    "executions[0].render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you wait for the training job to complete, let's take a look at the `run.py` code which was used to set it up. This script takes all of the input parameters, including the dataset locations and hyperparameters which you saved to JSON files earlier in this notebook, and uses them to generate the templates which the pipeline needs to run the training job. It *does not* create the actual Step Functions instance - it only generates the templates which define the Step Functions workflow, as well as the CloudFormation input templates which CodePipeline uses to instantiate the Step Functions instance.\n",
    "\n",
    "Step-by-step, the script does the following:\n",
    "\n",
    "1. It collects all the input parameters it needs to generate the templates. This includes information about the environment container needed to run the training job, the input and output data locations, IAM roles needed by various components, encryption keys, and more. It then sets up some basic parameters like the AWS region and the function names.\n",
    "1. If the input parameters specify an environment container stored in ECR, it fetches that container. Otherwise, it fetches the URI of the AWS managed environment container needed for the training job.\n",
    "1. It reads the input data JSON file which you generated earlier in this notebook (and which was included in the zip source for the pipeline), thereby fetching the locations of the train, validation, and baseline data files. Then it formats more parameters which will be needed later in the script, including version IDs and output data locations.\n",
    "1. It reads the hyperparameter JSON file which you generated earlier in this notebook.\n",
    "1. It defines the Step Functions workflow, starting with the input schema, followed by each step of the workflow (i.e. Create Experiment, Baseline Job, Training Job), and finally combining those steps into a workflow graph. \n",
    "1. The workflow graph is saved to file, along with a file containing all of the input parameters saved according to the schema defined in the workflow.\n",
    "1. It saves parameters to file which will be used by CloudFormation to instantiate the Step Functions workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ../model/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and baseline jobs are complete (meaning they are displayed in a green color in the Step Functions workflow), you can inspect the experiment metrics. <a id=\"validation-results\"></a>\n",
    "\n",
    "TODO: Why are the baseline results NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import analytics\n",
    "experiment_name = 'mlops-{}'.format(model_name)\n",
    "model_analytics = analytics.ExperimentAnalytics(experiment_name=experiment_name)\n",
    "analytics_df = model_analytics.dataframe()\n",
    "\n",
    "if (analytics_df.shape[0] == 0):\n",
    "    raise(Exception('Please wait.  No training or baseline jobs'))\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100) # Increase column width to show full copmontent name\n",
    "cols = ['TrialComponentName', 'DisplayName', 'SageMaker.InstanceType', \n",
    "        'train:rmse - Last', 'validation:rmse - Last'] # return the last rmse for training and validation\n",
    "analytics_df[analytics_df.columns & cols].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dev Deployment\n",
    "\n",
    "When the pipeline has finished training a model, it automatically moves to the next step, where the model is deployed as an API endpoint. This endpoint is part of your dev deployment, therefore, in this section, you will run some tests on the endpoint to decide if you want to deploy this model into production.\n",
    "\n",
    "First, run the cell below to fetch the name of the SageMaker API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepipeline = boto3.client('codepipeline')\n",
    "\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\n",
    "    for stage in response['stageStates']:\n",
    "        if stage['stageName'] == stage_name:\n",
    "            return stage\n",
    "        \n",
    "# Get last execution id\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev:\n",
    "    raise(Exception('Please wait.  Deploy dev not started'))\n",
    "    \n",
    "execution_id = deploy_dev['latestExecution']['pipelineExecutionId']\n",
    "dev_endpoint_name = 'mlops-{}-dev-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('endpoint name: {}'.format(dev_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you moved through the previous section very quickly, you will need to wait until the dev endpoint has been successfully deployed and the pipeline is waiting for approval to deploy to production (see screenshot). It can take up to 10 minutes for SageMaker to create an API endpoint.  \n",
    "\n",
    "![Failed code pipeline](../docs/dev-deploy-ready.png)\n",
    "\n",
    "Alternatively, run the code below to check the status of your endpoint. Re-run it every few minutes until the status of the endpoint is 'InService'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        if response['EndpointStatus'] == 'InService':\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your endpoint is ready, let's write some code to run the test data (which you split off from the dataset and saved to file at the start of this notebook) through the endpoint for inference. The code below supports both v1 and v2 of the SageMaker SDK, but we recommend using v2 of the SDK in all of your future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    # Support SageMaker v2 SDK: https://sagemaker.readthedocs.io/en/stable/v2.html\n",
    "    from sagemaker.predictor import Predictor\n",
    "    from sagemaker.serializers import CSVSerializer\n",
    "    def get_predictor(endpoint_name):\n",
    "        xgb_predictor = Predictor(endpoint_name)\n",
    "        xgb_predictor.serializer = CSVSerializer()\n",
    "        return xgb_predictor\n",
    "except:\n",
    "    # Fallback to SageMaker v1.70 SDK\n",
    "    from sagemaker.predictor import RealTimePredictor, csv_serializer\n",
    "    def get_predictor(endpoint_name):\n",
    "        xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "        xgb_predictor.content_type = 'text/csv'\n",
    "        xgb_predictor.serializer = csv_serializer\n",
    "        return xgb_predictor\n",
    "\n",
    "def predict(predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, round(data.shape[0] / float(rows)))\n",
    "    predictions = ''\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the predict function which was defined in the code above to run the test data through the endpoint and generate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\n",
    "predictions = predict(dev_predictor, test_df[test_df.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the predictions into a data frame, and join it with your test data. Then, calculate absolute error as the difference between the actual taxi fare and the predicted taxi fare. Display the results in a table, sorted by the highest absolute error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'total_amount_predictions': predictions })\n",
    "pred_df = test_df.join(pred_df) # Join on all\n",
    "pred_df['error'] = abs(pred_df['total_amount']-pred_df['total_amount_predictions'])\n",
    "\n",
    "pred_df.sort_values('error', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table it looks like there are some problems with very low fares predicted for low trip distances.\n",
    "\n",
    "Alternatively, you can also analyze the results by plotting the absolute error to visualize outliers. In this graph, we see that most of the outliers are cases where the model predicted a much lower fare than the actual fare. There are only a few outliers where the mode predicted a higher fare than the actual fare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pred_df, x='total_amount_predictions', y='total_amount', hue='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want one overall measure of quality for the model, you can calculate the root mean square error (RMSE) for the predicted fares compared to the actual fares. Compare this to the [results calculated on the validation set](#validation-results) at the end of the 'Inspect Training Job' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(pred_df):\n",
    "    return sqrt(mean_squared_error(pred_df['total_amount'], pred_df['total_amount_predictions']))\n",
    "\n",
    "print('RMSE: {}'.format(rmse(pred_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approve Deployment to Production\n",
    "\n",
    "If you are happy with the results of the model, you can go ahead and approve the model to be deployed into production. You can do so by clicking the 'Review' button in the CodePipeline UI (see screenshot), leaving a comment to explain why you approve this model, and clicking on 'Approve'. \n",
    "\n",
    "![Code pipeline](../docs/deploy-dev.png)\n",
    "\n",
    "Alternatively, you can create a Jupyter widget which enables you to comment and approve the model directly from this notebook. Run the cell below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def on_click(obj):\n",
    "    result = { 'summary': approval_text.value, 'status': obj.description }\n",
    "    response = codepipeline.put_approval_result(\n",
    "      pipelineName=pipeline_name,\n",
    "      stageName='DeployDev',\n",
    "      actionName='ApproveDeploy',\n",
    "      result=result,\n",
    "      token=approval_action['token']\n",
    "    )\n",
    "    button_box.close()\n",
    "    print(result)\n",
    "    \n",
    "# Create the widget if we are ready for approval\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev['actionStates'][-1]:\n",
    "    raise(Exception('Please wait.  Deploy dev not complete'))\n",
    "\n",
    "approval_action = deploy_dev['actionStates'][-1]['latestExecution']\n",
    "if approval_action['status'] == 'Succeeded':\n",
    "    print('Dev approved: {}'.format(approval_action['summary']))\n",
    "elif 'token' in approval_action:\n",
    "    approval_text = widgets.Text(placeholder='Optional approval message')   \n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style='success', icon='check')\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style='danger', icon='close')\n",
    "    approve_btn.on_click(on_click)\n",
    "    reject_btn.on_click(on_click)\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\n",
    "    display(button_box)\n",
    "else:\n",
    "    raise(Exception('Please wait. No dev approval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Production Deployment\n",
    "\n",
    "Within about a minute after approving the model deployment, you should see the pipeline start on the final step: deploying your model into production. In this section, you will check the deployment status and test the production endpoint after it has been deployed.\n",
    "\n",
    "![Code pipeline](../docs/deploy-production.png)\n",
    "\n",
    "This step of the pipeline uses CloudFormation to deploy a number of resources on your behalf. In particular, it creates:\n",
    "\n",
    "1. A production-ready SageMaker API endpoint for your model, with [data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html) enabled (to be used by SageMaker Model Monitor) and [autoscaling](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html) enabled.\n",
    "1. A [model monitoring schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html) which outputs the results to CloudWatch metrics, along with a [CloudWatch alarm](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html) which will notify you when a violation occurs. \n",
    "1. A CodeDeploy application which TODO\n",
    "\n",
    "![Code pipeline](../docs/cloud-formation.png)\n",
    "\n",
    "Let's check how the deployment is progressing. Use the code below to fetch the execution ID of the depoyment step. Then generate a table which lists the resources created by the CloudFormation stack and their creation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd or not 'latestExecution' in deploy_prd['actionStates'][0]:\n",
    "    raise(Exception('Please wait.  Deploy prd not started'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "def get_event_dataframe(events):\n",
    "    stack_cols = ['LogicalResourceId', 'ResourceStatus', 'ResourceStatusReason', 'Timestamp']\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna('')\n",
    "    stack_event_df['TimeAgo'] = (datetime.now(tzlocal())-stack_event_df['Timestamp'])\n",
    "    return stack_event_df.drop('Timestamp', axis=1)\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "stack_name = stack_name='{}-deploy-prd'.format(pipeline_name)\n",
    "print('stack name: {}'.format(stack_name))\n",
    "\n",
    "# Get latest stack events\n",
    "while True:\n",
    "    try:\n",
    "        response = cfn.describe_stack_events(StackName=stack_name)\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "    \n",
    "get_event_dataframe(response['StackEvents']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resource of most interest to us is the endpoint. This takes on average TODO minutes to deploy. Use the code below to fetch the name of the endpoint, then run a loop to wait for the endpoint to be fully deployed. You need the status to be 'InService'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_endpoint_name='mlops-{}-prd-{}'.format(model_name, execution_id)\n",
    "print('prod endpoint: {}'.format(prd_endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        # Wait until the endpoint is in service with data capture enabled\n",
    "        if response['EndpointStatus'] == 'InService' \\\n",
    "            and 'DataCaptureConfig' in response \\\n",
    "            and response['DataCaptureConfig']['EnableCapture']:\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the endpoint status is 'InService', you can continue. Earlier in this notebook, you created some code to send data to the dev endpoint. Reuse this code now to send a sample of the test data to the production endpoint. Since data capture is enabled on this endpoint, you want to send single records at a time, so the model monitor can map these records to the baseline. You will take a closer look at the model monitor [later on in this notebook](#Model-Monitor). For now, just check if you can send data to the endpoint and receive predictions in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\n",
    "sample_values = test_df[test_df.columns[1:]].sample(100).values\n",
    "predictions = predict(prd_predictor, sample_values, rows=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rest API\n",
    "\n",
    "Get back the deployment progress and rest API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_status(stack_name):\n",
    "    response = cfn.describe_stacks(StackName=stack_name)\n",
    "    if response['Stacks']:\n",
    "        stack = response['Stacks'][0]\n",
    "        outputs = None\n",
    "        if 'Outputs' in stack:\n",
    "            outputs = dict([(o['OutputKey'], o['OutputValue']) for o in stack['Outputs']])\n",
    "        return stack['StackStatus'], outputs \n",
    "\n",
    "outputs = None\n",
    "while True:\n",
    "    try:\n",
    "        status, outputs = get_stack_status(stack_name)\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        if outputs:\n",
    "            break\n",
    "        elif status.endswith('FAILED'):\n",
    "            raise(Exception('Stack status: {}'.format(status)))\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "if outputs:\n",
    "    print('deployment application: {}'.format(outputs['DeploymentApplication']))\n",
    "    print('rest api: {}'.format(outputs['RestApi']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the deployment application to see if its created and started to shift traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codedeploy/applications/{1}?region={0}\">Deployment Application</a>'.format(region, outputs['DeploymentApplication']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ping the REST endpoint to see which SageMaker endpoint it is hitting.  Press STOP when deployment complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "headers = {\"Content-type\": \"text/csv\"}\n",
    "payload = test_df[test_df.columns[1:]].head(1).to_csv(header=False, index=False).encode('utf-8')\n",
    "rest_api = outputs['RestApi']\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        resp = request.urlopen(request.Request(rest_api, data=payload, headers=headers))\n",
    "        print(\"Response code: %d: endpoint: %s\" % (resp.getcode(), resp.getheader('x-sagemaker-endpoint')))\n",
    "        status, outputs = get_stack_status(stack_name) \n",
    "        if status.endswith('COMPLETE'):\n",
    "            print('Deployment complete\\n')\n",
    "            break\n",
    "        elif status.endswith('FAILED'):\n",
    "            raise(Exception('Stack status: {}'.format(status)))\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Model Monitor\n",
    "\n",
    "When you prepared the datasets for model training at the start of this notebook, you saved a baseline dataset (a copy of the train dataset). Then, when you approved the model for deployment into production, the pipeline set up an API endpoint with data capture enabled and a model monitoring schedule. In this section, you will take a closer look at the model monitor results.\n",
    "\n",
    "To start off, fetch the latest production deployment execution ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd:\n",
    "    raise(Exception('Please wait.  Deploy prod not complete'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, SageMaker model monitor runs in SageMaker processing jobs. Use the execution ID to recreate the names of the processing job and the schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name='mlops-{}-pbl-{}'.format(model_name, execution_id)\n",
    "schedule_name='mlops-{}-pms-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('processing job name: {}'.format(processing_job_name))\n",
    "print('schedule name: {}'.format(schedule_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore baseline output\n",
    "\n",
    "Now fetch the baseline results from the processing job. This cell will throw an exception if the processing job is not complete - if that happens, just wait several minutes and try again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model_monitor import BaseliningJob, MonitoringExecution\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "baseline_job = BaseliningJob.from_processing_name(sagemaker_session, processing_job_name)\n",
    "status = baseline_job.describe()['ProcessingJobStatus']\n",
    "if status != 'Completed':\n",
    "    raise(Exception('Please wait. Processing job not complete, status: {}'.format(status)))\n",
    "    \n",
    "baseline_results_uri  = baseline_job.outputs[0].destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker model monitor generates two types of files. Take a look at the statistics file first. It calculates various statistics for each feature of the dataset, including the mean, standard deviation, minimum value, maximum value, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "schema_df = pd.json_normalize(baseline_statistics[\"features\"])\n",
    "schema_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the suggested constraints file. As the name implies, these are constraints which SageMaker model monitor recommends. If the live data which is sent to your production API endpoint violates these constraints, this indicates data drift, and model monitor can raise an alert or trigger a retraining. Of course, you can set different constraints based on the statistics which you viewed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_constraints = baseline_job.suggested_constraints().body_dict\n",
    "constraints_df = pd.json_normalize(baseline_constraints[\"features\"])\n",
    "constraints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View data capture\n",
    "\n",
    "When the \"Deploy Production\" stage of the MLOps pipeline deploys a SageMaker endpoint, it also enables data capture. This means the incoming requests to the endpoint, as well as the results from the ML model, are stored in an S3 location. Model monitor can analyze this data and compare it to the baseline to ensure that no constraints are violated. \n",
    "\n",
    "Use the code below to check how many files have been created by the data capture, and view the latest file in detail. Note, data capture relies on data being sent to the production endpoint. If you don't see any files yet, wait several minutes and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "data_capture_logs_uri = 's3://{}/{}/datacapture/{}'.format(bucket, model_name, prd_endpoint_name)\n",
    "\n",
    "capture_files = S3Downloader.list(data_capture_logs_uri)\n",
    "print('Found {} files'.format(len(capture_files)))\n",
    "\n",
    "if capture_files:\n",
    "    # Get the first line of the most recent file    \n",
    "    event = json.loads(S3Downloader.read_file(capture_files[-1]).split('\\n')[0])\n",
    "    print('\\nLast file:\\n{}'.format(json.dumps(event, indent=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View monitoring schedule\n",
    "\n",
    "There are some useful functions for plotting and rendering distribution statistics or constraint violations provided in a `utils` file in the [SageMaker Examples GitHub](https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker_model_monitor/visualization). Grab a copy of this code to use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O utils.py --quiet https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\n",
    "import utils as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [minimum scheduled run time](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html) for model monitor is one hour, which means you will need to wait at least an hour to see any results. Use the code below to check the schedule status and list the next run. If you are completing this notebook as part of a workshop, your host will have activities which you can complete while you wait. Alternatively, you can spend some time looking at the CloudFormation template (`assets/deploy-model-prod.yml`) which was used to deploy the production endpoint, and understand the Lambda functions which it deployed along with an API Gateway instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "response = sm.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "print('Schedule Status: {}'.format(response['MonitoringScheduleStatus']))\n",
    "\n",
    "now = datetime.now(tzlocal())\n",
    "next_hour = (now+timedelta(hours=1)).replace(minute=0)\n",
    "scheduled_diff = (next_hour-now).seconds//60\n",
    "print('Next schedule in {} minutes'.format(scheduled_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of minutes after the model monitoring schedule has run, you can use the code below to fetch the latest schedule status. The run may have found violations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_arn = None\n",
    "\n",
    "while processing_job_arn == None:\n",
    "    try:\n",
    "        response = sm.list_monitoring_executions(MonitoringScheduleName=schedule_name)\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    for mon in response['MonitoringExecutionSummaries']:\n",
    "        status = mon['MonitoringExecutionStatus']\n",
    "        now = datetime.now(tzlocal())\n",
    "        created_diff = (now-mon['CreationTime']).seconds//60\n",
    "        print('Schedule status: {}, Created: {} minutes ago'.format(status, created_diff))\n",
    "        if status in ['Completed', 'CompletedWithViolations']:\n",
    "            processing_job_arn = mon['ProcessingJobArn']\n",
    "            break\n",
    "        if status == 'InProgress':\n",
    "            break\n",
    "    else:\n",
    "        raise(Exception('Please wait.  No Schedules created'))\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View monitoring results\n",
    "\n",
    "Once the model monitoring schedule has had a chance to run at least once, you can take a look at the results. First, load the monitoring execution results from the latest scheduled run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(), \n",
    "                                                    processing_job_arn=processing_job_arn)\n",
    "exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\n",
    "exec_results_uri = execution.output.destination\n",
    "\n",
    "print('Monitoring Execution results: {}'.format(exec_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the files which have been saved in the S3 output location. If violations were found, you should see a constraint violations file in addition to the statistics and constraints file which you viewed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $exec_results_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fetch the monitoring statistics and violations. Then use the utils code to visualize the results in a table. It will highlight any baseline drift found by the model monitor. Drift can happen for categorical features (e.g. passenger count should be an integer value, not a fractional value) or for numerical features (e.g. total fare amount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline and monitoring statistics & violations\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "execution_statistics = execution.statistics().body_dict\n",
    "violations = execution.constraint_violations().body_dict['violations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, \n",
    "                     latest_statistics=execution_statistics, \n",
    "                     violations=violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger retraining\n",
    "\n",
    "The CodePipeline instance is configured with [CloudWatch Events](https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source.html) to start the pipeline for retraining when the drift detection triggers specific metric alarms.\n",
    "\n",
    "You can simulate drift by putting a metric value of `0.5` directly into CloudWatch, which is above the threshold of `0.2`.  This will trigger the alarm, and start the code pipeline retraining.\n",
    "\n",
    "![Code pipeline](../docs/cloudwatch-alarm.png)\n",
    "\n",
    "Run the code below to trigger the metric alarm. The cell output will be a link to CloudWatch, where you can see the alarm (similar to the screenshot above), and a link to CodePipeline which you will see run again. Note that it can take several minutes for everything to trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Put a new metric to trigger an alaram\n",
    "response = cloudwatch.put_metric_data(\n",
    "    Namespace='aws/sagemaker/Endpoints/data-metrics',\n",
    "    MetricData=[\n",
    "        {\n",
    "            'MetricName': 'feature_baseline_drift_total_amount',\n",
    "            'Dimensions': [\n",
    "                {\n",
    "                    'Name': 'MonitoringSchedule',\n",
    "                    'Value': schedule_name\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'Endpoint',\n",
    "                    'Value': prd_endpoint_name\n",
    "                },\n",
    "            ],\n",
    "            'Timestamp': datetime.now(),\n",
    "            'Value': 0.5, # This is over the configured threshold of 0.2\n",
    "            'Unit': 'None'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(response)\n",
    "\n",
    "# Output a html link to the cloudwatch dashboard\n",
    "alarm_name = 'mlops-nyctaxi-metric-gt-threshold'\n",
    "HTML('''<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#alarmsV2:alarm/{1}\">CloudWatch Alarm</a> starts \n",
    "     <a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{2}/view?region={0}\">Code Pipeline</a>'''.format(region, alarm_name, pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch Monitoring\n",
    "\n",
    "AWS [CloudWatch Synthetics](https://aws.amazon.com/blogs/aws/new-use-cloudwatch-synthetics-to-monitor-sites-api-endpoints-web-workflows-and-more/) provides allow you to setup a canary to test that your API is returning an expected value on a regular interval.  This is a great way to validate that the blue/green deployment is not causing any downtime for our end-users.\n",
    "\n",
    "### Create Canary\n",
    "\n",
    "Let's setup a \"canary\" to continously test the production API, and a dashboard to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from string import Template\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "\n",
    "# Format the canary_js with rest_api and payload\n",
    "rest_url = urlparse(rest_api)\n",
    "\n",
    "with open('canary.js') as f:\n",
    "    canary_js = Template(f.read()).substitute(hostname=rest_url.netloc, path=rest_url.path, \n",
    "                                              data=payload.decode('utf-8').strip())\n",
    "# Write the zip file\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'w') as zf:\n",
    "    zip_path = 'nodejs/node_modules/apiCanaryBlueprint.js' # Set a valid path\n",
    "    zip_info = zipfile.ZipInfo(zip_path)\n",
    "    zip_info.external_attr = 0o0755 << 16 # Ensure the file is readable\n",
    "    zf.writestr(zip_info, canary_js)\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "# Create the canary\n",
    "synth = boto3.client('synthetics')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_canary_uri = 's3://{}/{}'.format(artifact_bucket, model_name)\n",
    "canary_name = 'mlops-{}'.format(model_name)\n",
    "\n",
    "response = synth.create_canary(\n",
    "    Name=canary_name,\n",
    "    Code={\n",
    "        'ZipFile': bytearray(zip_buffer.read()),\n",
    "        'Handler': 'apiCanaryBlueprint.handler'\n",
    "    },\n",
    "    ArtifactS3Location=s3_canary_uri,\n",
    "    ExecutionRoleArn=role,\n",
    "    Schedule={ \n",
    "        'Expression': 'rate(10 minutes)', \n",
    "        'DurationInSeconds': 0 },\n",
    "    RunConfig={\n",
    "        'TimeoutInSeconds': 60,\n",
    "        'MemoryInMB': 960\n",
    "    },\n",
    "    SuccessRetentionPeriodInDays=31,\n",
    "    FailureRetentionPeriodInDays=31,\n",
    "    RuntimeVersion='syn-nodejs-2.0',\n",
    ")\n",
    "\n",
    "print('Creating canary: {}'.format(canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the a CloudWatch alarm when success percent drops below 90% for that canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "canary_alarm_name = '{}-synth-lt-threshold'.format(canary_name)\n",
    "\n",
    "response = cloudwatch.put_metric_alarm(\n",
    "    AlarmName=canary_alarm_name,\n",
    "    ComparisonOperator='LessThanThreshold',\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Period=600, # 10 minute interval\n",
    "    Statistic='Average',\n",
    "    Threshold=90.0,\n",
    "    ActionsEnabled=False,\n",
    "    AlarmDescription='SuccessPercent LessThanThreshold 90%',\n",
    "    Namespace='CloudWatchSynthetics',\n",
    "    MetricName='SuccessPercent',\n",
    "    Dimensions=[\n",
    "        {\n",
    "          'Name': 'CanaryName',\n",
    "          'Value': canary_name\n",
    "        },\n",
    "    ],\n",
    "    Unit='Seconds'\n",
    ")\n",
    "\n",
    "print('Creating alarm: {}'.format(canary_alarm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for the canary to be read, then start it and wait until running.  The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        response = synth.get_canary(Name=canary_name)\n",
    "        status = response['Canary']['Status']['State']    \n",
    "        print('Canary status: {}'.format(status))\n",
    "        if status == 'ERROR':\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \n",
    "        elif status == 'READY':\n",
    "            synth.start_canary(Name=canary_name)\n",
    "        elif status == 'RUNNING':\n",
    "            break        \n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print('No canary found.')\n",
    "            break\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "# Output a html link to the cloudwatch console\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#synthetics:canary/detail/{1}\">CloudWatch Canary</a>'.format(region, canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dashboard\n",
    "\n",
    "Finally let's create a AWS CloudWatch Dashboard to visualize the key performane metrics and alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity().get('Account')\n",
    "dashboard_name = 'mlops-{}'.format(model_name)\n",
    "\n",
    "with open('dashboard.json') as f:\n",
    "    dashboard_body = Template(f.read()).substitute(region=region, account_id=account_id, model_name=model_name)\n",
    "    response = cloudwatch.put_dashboard(\n",
    "        DashboardName=dashboard_name,\n",
    "        DashboardBody=dashboard_body\n",
    "    )\n",
    "\n",
    "# Output a html link to the cloudwatch dashboard\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#dashboards:name={1}\">CloudWatch Dashboard</a>'.format(region, canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "First delete the stacks used as part of the pipeline for deployment. For a model name of **nyctaxi** these would be:\n",
    "\n",
    "* *nyctaxi*-deploy-prd\n",
    "* *nyctaxi*-deploy-dev\n",
    "\n",
    "Then use the following code to delete the monitoring schedules attached to the production endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_schedules = session.list_monitoring_schedules(endpoint_name=prd_endpoint_name)\n",
    "schedule_names = [schedule['MonitoringScheduleName'] for schedule in monitoring_schedules['MonitoringScheduleSummaries']]\n",
    "for name in schedule_names:\n",
    "    session.delete_monitoring_schedule(monitoring_schedule_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will stop and delete the canary you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        response = synth.get_canary(Name=canary_name)\n",
    "        status = response['Canary']['Status']['State']    \n",
    "        print('Canary status: {}'.format(status))\n",
    "        if status == 'ERROR':\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \n",
    "        elif status == 'STOPPED':\n",
    "            synth.delete_canary(Name=canary_name)\n",
    "        elif status == 'RUNNING':\n",
    "            synth.stop_canary(Name=canary_name)\n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print('Canary succesfully deleted.')\n",
    "            break\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will delete the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch.delete_alarms(AlarmNames=[canary_alarm_name])\n",
    "print('Alarm deleted')\n",
    "\n",
    "cloudwatch.delete_dashboards(DashboardNames=[dashboard_name])\n",
    "print('Dashboard deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close this notebook and delete the remaining CloudFormation stacks related to this demo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
