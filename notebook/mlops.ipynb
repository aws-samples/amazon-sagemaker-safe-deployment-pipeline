{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Demo\n",
    "\n",
    "Julian Bright, Machine Learning Specialist @ Amazon Web Services\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook you will automate an MLOps pipeline build, train, deploy and monitor an XGBoost regression model for predicting the expected taxi fare using the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/).\n",
    "\n",
    "This notebook will take you through a series of steps to execute the AWS CodePipeline stage as depicted below:\n",
    "\n",
    "![Code pipeline](../docs/code-pipeline.png)\n",
    "\n",
    "Following a series of steps to trigger demo\n",
    "\n",
    "1. [Data Prep](#Data-Prep)\n",
    "2. [Start Build](#Start-Build)\n",
    "3. [Wait for Training Job](#Wait-for-Training-Job)\n",
    "4. [Test Dev Deployment](#Test-Dev-Deployment)\n",
    "5. [Approve Prod Endpoint](#Approve-Prod-Deployment)\n",
    "6. [Test Prod Deployment](#Test-Prod-Deployment)\n",
    "7. [Model Monitoring](#Model-Monitoring)\n",
    "8. [CloudWatch Monitoring](#CloudWatch-Monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the latest sagemaker and boto3 SDKs\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" tqdm\n",
    "!{sys.executable} -m pip show sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Download a sample of the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "parse_dates= ['lpep_dropoff_datetime', 'lpep_pickup_datetime']\n",
    "trip_df = pd.read_csv('nyc-tlc.csv', parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering to convert dates and add derived duration in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df['duration_minutes'] = (trip_df['lpep_dropoff_datetime'] - trip_df['lpep_pickup_datetime']).dt.seconds/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a sample of columns for our machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['total_amount', 'duration_minutes', 'passenger_count', 'trip_distance']\n",
    "data_df = trip_df[cols]\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude any outlines, dropping any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[(data_df.total_amount > 0) & (data_df.total_amount < 200) & \n",
    "                  (data_df.duration_minutes > 0) & (data_df.duration_minutes < 120) & \n",
    "                  (data_df.trip_distance > 0) & (data_df.trip_distance < 1000) & \n",
    "                  (data_df.passenger_count > 0)].dropna()\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize \n",
    "\n",
    "Sample and plot distribution of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data_df.sample(1000)\n",
    "sample_df.hist(bins=100, layout=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot duration vs trip distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.plot.scatter(x='duration_minutes', y='trip_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot duration vs total amount and we see a similar pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.plot.scatter(x='duration_minutes', y='total_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to split the dataset into train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\n",
    "\n",
    "# Set the index for our test dataframe\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('split train: {}, val: {}, test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files as CSV including baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_cols = ['total_amount', 'duration_minutes','passenger_count','trip_distance']\n",
    "train_df.to_csv('train.csv', index=False, header=False)\n",
    "val_df.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Save test and baseline with headers\n",
    "test_df.to_csv('test.csv', index=False, header=True)\n",
    "train_df.to_csv('baseline.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Specify data previx version\n",
    "prefix = 'nyc-tlc/v1'\n",
    "\n",
    "s3_train_uri = session.upload_data('train.csv', bucket, prefix + '/data/training')\n",
    "s3_val_uri = session.upload_data('validation.csv', bucket, prefix + '/data/validation')\n",
    "s3_baseline_uri = session.upload_data('baseline.csv', bucket, prefix + '/data/baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Build\n",
    "\n",
    "Load variables from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\n",
    "pipeline_name = os.environ['PIPELINE_NAME']\n",
    "model_name = os.environ['MODEL_NAME']\n",
    "\n",
    "print('region: {}'.format(region))\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\n",
    "print('pipeline: {}'.format(pipeline_name))\n",
    "print('model name: {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data source meta data to trigger a new build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "input_data = {\n",
    "    'TrainingUri': s3_train_uri,\n",
    "    'ValidationUri': s3_val_uri,\n",
    "    'BaselineUri': s3_baseline_uri\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_round': 50\n",
    "}\n",
    "\n",
    "data_source_key = '{}/data-source.zip'.format(pipeline_name)\n",
    "\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\n",
    "    zf.writestr('inputData.json', json.dumps(input_data))\n",
    "    zf.writestr('hyperparameters.json', json.dumps(hyperparameters))\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Training Job\n",
    "\n",
    "Follow the code pipeline to wait until the training job is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(region, pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are waiting for the code pipeline to run, let's take a look at the model `run.py` code.  \n",
    "\n",
    "* We can see the XGBoost SageMaker estimator define in the `get_training_params` method.\n",
    "* The `training_uri` and  `validation_uri` are loaded from the `inputData.json` file in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ../model/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and baseline job is complete we can inspect the exeriment metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import analytics\n",
    "model_analytics = analytics.ExperimentAnalytics(experiment_name=model_name)\n",
    "analytics_df = model_analytics.dataframe()\n",
    "\n",
    "if (analytics_df.shape[0] == 0):\n",
    "    raise(Exception('Please wait.  No training or baseline jobs'))\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100) # Increase column width to show full copmontent name\n",
    "cols = ['TrialComponentName', 'DisplayName', 'SageMaker.InstanceType', \n",
    "        'train:rmse - Last', 'validation:rmse - Last'] # return the last rmse for training and validation\n",
    "analytics_df[analytics_df.columns & cols].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dev Deployment\n",
    "\n",
    "One the endpoint has been deployed and awaiting approval, we can begin some tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepipeline = boto3.client('codepipeline')\n",
    "\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\n",
    "    for stage in response['stageStates']:\n",
    "        if stage['stageName'] == stage_name:\n",
    "            return stage\n",
    "        \n",
    "# Get last execution id\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev:\n",
    "    raise(Exception('Please wait.  Deploy dev not started'))\n",
    "    \n",
    "execution_id = deploy_dev['latestExecution']['pipelineExecutionId']\n",
    "dev_endpoint_name = 'mlops-{}-dev-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('endpoint name: {}'.format(dev_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the dev endpoint is in service (this can take up to 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        if response['EndpointStatus'] == 'InService':\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    # Support SageMaker v2 SDK: https://sagemaker.readthedocs.io/en/stable/v2.html\n",
    "    from sagemaker.predictor import Predictor\n",
    "    from sagemaker.serializers import CSVSerializer\n",
    "    def get_predictor(endpoint_name):\n",
    "        xgb_predictor = Predictor(endpoint_name)\n",
    "        xgb_predictor.serializer = CSVSerializer()\n",
    "        return xgb_predictor\n",
    "except:\n",
    "    # Fallback to SageMaker v1.70 SDK\n",
    "    from sagemaker.predictor import RealTimePredictor, csv_serializer\n",
    "    def get_predictor(endpoint_name):\n",
    "        xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "        xgb_predictor.content_type = 'text/csv'\n",
    "        xgb_predictor.serializer = csv_serializer\n",
    "        return xgb_predictor\n",
    "\n",
    "def predict(predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, round(data.shape[0] / float(rows)))\n",
    "    predictions = ''\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the dev endpoint with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\n",
    "predictions = predict(dev_predictor, test_df[test_df.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the response into a data frame, and join with predictions to calculate absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'total_amount_predictions': predictions })\n",
    "pred_df = test_df.join(pred_df) # Join on all\n",
    "pred_df['error'] = abs(pred_df['total_amount']-pred_df['total_amount_predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the largest errors are high predict amount for low trip distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.sort_values('error', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the absoulte error between the total amount vs predicted to visualise outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pred_df.tail(1000).plot.scatter(x='total_amount_predictions', y='total_amount', \n",
    "                                     c='error', title='actual amount vs pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the root means square error for the predicted total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(pred_df):\n",
    "    return sqrt(mean_squared_error(pred_df['total_amount'], pred_df['total_amount_predictions']))\n",
    "\n",
    "print('RMSE: {}'.format(rmse(pred_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approve Prod Deployment\n",
    "\n",
    "If we are happy with this metric, we can go ahead and approve with the widget below, or manually in the CodePipeline  by clicking the \"Review\" button.\n",
    "\n",
    "![Code pipeline](../docs/deploy-dev.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def on_click(obj):\n",
    "    result = { 'summary': approval_text.value, 'status': obj.description }\n",
    "    response = codepipeline.put_approval_result(\n",
    "      pipelineName=pipeline_name,\n",
    "      stageName='DeployDev',\n",
    "      actionName='ApproveDeploy',\n",
    "      result=result,\n",
    "      token=approval_action['token']\n",
    "    )\n",
    "    button_box.close()\n",
    "    print(result)\n",
    "    \n",
    "# Create the widget if we are ready for approval\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev['actionStates'][-1]:\n",
    "    raise(Exception('Please wait.  Deploy dev not complete'))\n",
    "\n",
    "approval_action = deploy_dev['actionStates'][-1]['latestExecution']\n",
    "if approval_action['status'] == 'Succeeded':\n",
    "    print('Dev approved: {}'.format(approval_action['summary']))\n",
    "elif 'token' in approval_action:\n",
    "    approval_text = widgets.Text(placeholder='Optional approval message')   \n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style='success', icon='check')\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style='danger', icon='close')\n",
    "    approve_btn.on_click(on_click)\n",
    "    reject_btn.on_click(on_click)\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\n",
    "    display(button_box)\n",
    "else:\n",
    "    raise(Exception('Please wait.  No dev approval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prod Deployment\n",
    "\n",
    "The prod deployment will start shortly after approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd or not 'latestExecution' in deploy_prd['actionStates'][0]:\n",
    "    raise(Exception('Please wait.  Deploy prd not started'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production deployment is managed through a CloudFormation stack which performs the following:\n",
    "\n",
    "1. Creates SageMaker Endpoint with Data Capture and AutoScaling enabled\n",
    "2. Creates Model Monitoring Schedule with CloudWatch Alarm\n",
    "3. Deploys an API Gateway Lambda with AWS Code Deploy\n",
    "\n",
    "![Code pipeline](../docs/cloud-formation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the last events and how long ago they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "def get_event_dataframe(events):\n",
    "    stack_cols = ['LogicalResourceId', 'ResourceStatus', 'ResourceStatusReason', 'Timestamp']\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna('')\n",
    "    stack_event_df['TimeAgo'] = (datetime.now(tzlocal())-stack_event_df['Timestamp'])\n",
    "    return stack_event_df.drop('Timestamp', axis=1)\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "stack_name = stack_name='{}-deploy-prd'.format(pipeline_name)\n",
    "print('stack name: {}'.format(stack_name))\n",
    "\n",
    "# Get latest stack events\n",
    "while True:\n",
    "    try:\n",
    "        response = cfn.describe_stack_events(StackName=stack_name)\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "    \n",
    "get_event_dataframe(response['StackEvents']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can send some traffic to the production endpoint now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_endpoint_name='mlops-{}-prd-{}'.format(model_name, execution_id)\n",
    "print('prod endpoint: {}'.format(prd_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the endpoint has finishing updated before we send some traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        # Wait until the endpoint is in service with data capture enabled\n",
    "        if response['EndpointStatus'] == 'InService' \\\n",
    "            and 'DataCaptureConfig' in response \\\n",
    "            and response['DataCaptureConfig']['EnableCapture']:\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send some inference to production endpoint now that data capture is enabled.  Use single records to that monitoring schedule can map to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\n",
    "sample_values = test_df[test_df.columns[1:]].sample(100).values\n",
    "predictions = predict(prd_predictor, sample_values, rows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rest API\n",
    "\n",
    "Get back the deployment progress and rest API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_status(stack_name):\n",
    "    response = cfn.describe_stacks(StackName=stack_name)\n",
    "    if response['Stacks']:\n",
    "        stack = response['Stacks'][0]\n",
    "        outputs = None\n",
    "        if 'Outputs' in stack:\n",
    "            outputs = dict([(o['OutputKey'], o['OutputValue']) for o in stack['Outputs']])\n",
    "        return stack['StackStatus'], outputs \n",
    "\n",
    "outputs = None\n",
    "while True:\n",
    "    try:\n",
    "        status, outputs = get_stack_status(stack_name)\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        if outputs:\n",
    "            break\n",
    "        elif status.endswith('FAILED'):\n",
    "            raise(Exception('Stack status: {}'.format(status)))\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "if outputs:\n",
    "    print('deployment application: {}'.format(outputs['DeploymentApplication']))\n",
    "    print('rest api: {}'.format(outputs['RestApi']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the deployment application to see if its created and started to shift traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codedeploy/applications/{1}?region={0}\">Deployment Application</a>'.format(region, outputs['DeploymentApplication']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ping the REST endpoint to see which SageMaker endpoint it is hitting.  Press STOP when deployment complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "headers = {\"Content-type\": \"text/csv\"}\n",
    "payload = test_df[test_df.columns[1:]].head(1).to_csv(header=False, index=False).encode('utf-8')\n",
    "rest_api = outputs['RestApi']\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        resp = request.urlopen(request.Request(rest_api, data=payload, headers=headers))\n",
    "        print(\"Response code: %d: endpoint: %s\" % (resp.getcode(), resp.getheader('x-sagemaker-endpoint')))\n",
    "        status, outputs = get_stack_status(stack_name) \n",
    "        if status.endswith('COMPLETE'):\n",
    "            print('Deployment complete\\n')\n",
    "            break\n",
    "        elif status.endswith('FAILED'):\n",
    "            raise(Exception('Stack status: {}'.format(status)))\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitor\n",
    "\n",
    "Get the latest production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd:\n",
    "    raise(Exception('Please wait.  Deploy prd not complete'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Load baseline processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name='mlops-{}-pbl-{}'.format(model_name, execution_id)\n",
    "schedule_name='mlops-{}-pms-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('processing job name: {}'.format(processing_job_name))\n",
    "print('schedule name: {}'.format(schedule_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model_monitor import BaseliningJob, MonitoringExecution\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "baseline_job = BaseliningJob.from_processing_name(sagemaker_session, processing_job_name)\n",
    "status = baseline_job.describe()['ProcessingJobStatus']\n",
    "if status != 'Completed':\n",
    "    raise(Exception('Please wait. Processing job not complete, status: {}'.format(status)))\n",
    "    \n",
    "baseline_results_uri  = baseline_job.outputs[0].destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "schema_df = pd.json_normalize(baseline_statistics[\"features\"])\n",
    "schema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_constraints = baseline_job.suggested_constraints().body_dict\n",
    "constraints_df = pd.json_normalize(baseline_constraints[\"features\"])\n",
    "constraints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data Capture\n",
    "\n",
    "Get the list of data capture files form the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "data_capture_logs_uri = 's3://{}/{}/datacapture/{}'.format(bucket, model_name, prd_endpoint_name)\n",
    "\n",
    "capture_files = S3Downloader.list(data_capture_logs_uri)\n",
    "print('Found {} files'.format(len(capture_files)))\n",
    "\n",
    "if capture_files:\n",
    "    # Get the first line of the most recent file    \n",
    "    event = json.loads(S3Downloader.read_file(capture_files[-1]).split('\\n')[0])\n",
    "    print('\\nLast file:\\n{}'.format(json.dumps(event, indent=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Monitoring Schedule\n",
    "\n",
    "The functions for plotting and rendering distribution statistics or constraint violations are implemented in a `utils` file so let's grab that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O utils.py --quiet https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\n",
    "import utils as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schedule status, and when the next hourly run is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "response = sm.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "print('Schedule Status: {}'.format(response['MonitoringScheduleStatus']))\n",
    "\n",
    "now = datetime.now(tzlocal())\n",
    "next_hour = (now+timedelta(hours=1)).replace(minute=0)\n",
    "scheduled_diff = (next_hour-now).seconds//60\n",
    "print('Next schedule in {} minutes'.format(scheduled_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the latest completed monitoring schedule (which may have violations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_arn = None\n",
    "\n",
    "while processing_job_arn == None:\n",
    "    try:\n",
    "        response = sm.list_monitoring_executions(MonitoringScheduleName=schedule_name)\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    for mon in response['MonitoringExecutionSummaries']:\n",
    "        status = mon['MonitoringExecutionStatus']\n",
    "        now = datetime.now(tzlocal())\n",
    "        created_diff = (now-mon['CreationTime']).seconds//60\n",
    "        print('Schedule status: {}, Created: {} minutes ago'.format(status, created_diff))\n",
    "        if status in ['Completed', 'CompletedWithViolations']:\n",
    "            processing_job_arn = mon['ProcessingJobArn']\n",
    "            break\n",
    "        if status == 'InProgress':\n",
    "            break\n",
    "    else:\n",
    "        raise(Exception('Please wait.  No Schedules created'))\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the monitoring execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(), \n",
    "                                                    processing_job_arn=processing_job_arn)\n",
    "exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\n",
    "exec_results_uri = execution.output.destination\n",
    "\n",
    "print('Monitoring Execution results: {}'.format(exec_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the constraints, statistics and violations if they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $exec_results_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline and monitoring statistics & violations\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "execution_statistics = execution.statistics().body_dict\n",
    "violations = execution.constraint_violations().body_dict['violations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, \n",
    "                     latest_statistics=execution_statistics, \n",
    "                     violations=violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch Monitoring\n",
    "\n",
    "AWS [CloudWatch Synthetics](https://aws.amazon.com/blogs/aws/new-use-cloudwatch-synthetics-to-monitor-sites-api-endpoints-web-workflows-and-more/) provides allow you to setup a canary to test that your API is returning an expected value on a regular interval.  This is a great way to validate that the blue/green deployment is not causing any downtime for our end-users.\n",
    "\n",
    "### Create Canary\n",
    "\n",
    "Let's setup a \"canary\" to continously test the production API, and a dashboard to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from string import Template\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "\n",
    "# Format the canary_js with rest_api and payload\n",
    "rest_url = urlparse(rest_api)\n",
    "\n",
    "with open('canary.js') as f:\n",
    "    canary_js = Template(f.read()).substitute(hostname=rest_url.netloc, path=rest_url.path, \n",
    "                                              data=payload.decode('utf-8').strip())\n",
    "# Write the zip file\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'w') as zf:\n",
    "    zip_path = 'nodejs/node_modules/apiCanaryBlueprint.js' # Set a valid path\n",
    "    zip_info = zipfile.ZipInfo(zip_path)\n",
    "    zip_info.external_attr = 0o0755 << 16 # Ensure the file is readable\n",
    "    zf.writestr(zip_info, canary_js)\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "# Create the canary\n",
    "synth = boto3.client('synthetics')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_canary_uri = 's3://{}/{}'.format(artifact_bucket, model_name)\n",
    "canary_name = 'mlops-{}'.format(model_name)\n",
    "\n",
    "response = synth.create_canary(\n",
    "    Name=canary_name,\n",
    "    Code={\n",
    "        'ZipFile': bytearray(zip_buffer.read()),\n",
    "        'Handler': 'apiCanaryBlueprint.handler'\n",
    "    },\n",
    "    ArtifactS3Location=s3_canary_uri,\n",
    "    ExecutionRoleArn=role,\n",
    "    Schedule={ \n",
    "        'Expression': 'rate(10 minutes)', \n",
    "        'DurationInSeconds': 0 },\n",
    "    RunConfig={\n",
    "        'TimeoutInSeconds': 60,\n",
    "        'MemoryInMB': 960\n",
    "    },\n",
    "    SuccessRetentionPeriodInDays=31,\n",
    "    FailureRetentionPeriodInDays=31,\n",
    "    RuntimeVersion='syn-1.0',\n",
    ")\n",
    "\n",
    "print('Creating canary: {}'.format(canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the a CloudWatch alarm when success percent drops below 90% for that canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "canary_alarm_name = '{}-synth-lt-threshold'.format(canary_name)\n",
    "\n",
    "response = cloudwatch.put_metric_alarm(\n",
    "    AlarmName=canary_alarm_name,\n",
    "    ComparisonOperator='LessThanThreshold',\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Period=600, # 10 minute interval\n",
    "    Statistic='Average',\n",
    "    Threshold=90.0,\n",
    "    ActionsEnabled=False,\n",
    "    AlarmDescription='SuccessPercent LessThanThreshold 90%',\n",
    "    Namespace='CloudWatchSynthetics',\n",
    "    MetricName='SuccessPercent',\n",
    "    Dimensions=[\n",
    "        {\n",
    "          'Name': 'CanaryName',\n",
    "          'Value': canary_name\n",
    "        },\n",
    "    ],\n",
    "    Unit='Seconds'\n",
    ")\n",
    "\n",
    "print('Creating alarm: {}'.format(canary_alarm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for the canary to be read, then start it and wait until running.  The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        response = synth.get_canary(Name=canary_name)\n",
    "        status = response['Canary']['Status']['State']    \n",
    "        print('Canary status: {}'.format(status))\n",
    "        if status == 'ERROR':\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \n",
    "        elif status == 'READY':\n",
    "            synth.start_canary(Name=canary_name)\n",
    "        elif status == 'RUNNING':\n",
    "            break        \n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print('No canary found.')\n",
    "            break\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "# Output a html link to the cloudwatch console\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#synthetics:canary/detail/{1}\">CloudWatch Canary</a>'.format(region, canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dashboard\n",
    "\n",
    "Finally let's create a AWS CloudWatch Dashboard to visualize the key performane metrics and alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity().get('Account')\n",
    "dashboard_name = 'mlops-{}'.format(model_name)\n",
    "\n",
    "with open('dashboard.json') as f:\n",
    "    dashboard_body = Template(f.read()).substitute(region=region, account_id=account_id, model_name=model_name)\n",
    "    response = cloudwatch.put_dashboard(\n",
    "        DashboardName=dashboard_name,\n",
    "        DashboardBody=dashboard_body\n",
    "    )\n",
    "\n",
    "# Output a html link to the cloudwatch dashboard\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#dashboards:name={1}\">CloudWatch Dashboard</a>'.format(region, canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Retraining\n",
    "\n",
    "Our CodePipeline is configured with a [CloudWatch Events](https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source.html) to start our pipeline for retraining when the drift detection metric alrams.\n",
    "\n",
    "We can simulate drift by putting metric `0.5` which is above the threshold of `0.2`.  This will trigger the alarm, and start the code pipeline retraining.\n",
    "\n",
    "Click through to the Alarm and CodePipeline with the links below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Put a new metric to trigger an alaram\n",
    "response = cloudwatch.put_metric_data(\n",
    "    Namespace='aws/sagemaker/Endpoints/data-metrics',\n",
    "    MetricData=[\n",
    "        {\n",
    "            'MetricName': 'feature_baseline_drift_total_amount',\n",
    "            'Dimensions': [\n",
    "                {\n",
    "                    'Name': 'MonitoringSchedule',\n",
    "                    'Value': schedule_name\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'Endpoint',\n",
    "                    'Value': prd_endpoint_name\n",
    "                },\n",
    "            ],\n",
    "            'Timestamp': datetime.now(),\n",
    "            'Value': 0.5, # This is over the configured threshold of 0.2\n",
    "            'Unit': 'None'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output a html link to the cloudwatch dashboard\n",
    "alarm_name = 'mlops-nyctaxi-metric-gt-threshold'\n",
    "HTML('''<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#alarmsV2:alarm/{1}\">CloudWatch Alarm</a> starts \n",
    "     <a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{2}/view?region={0}\">Code Pipeline</a>'''.format(region, alarm_name, pipeline_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "First delete the stacks used as part of the pipeline for deployment, training job and suggest baseline.  For a model name of **nyctaxi** that would be.\n",
    "\n",
    "* *nyctaxi*-devploy-prd\n",
    "* *nyctaxi*-devploy-dev\n",
    "* *nyctaxi*-training-job\n",
    "* *nyctaxi*-suggest-baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow code will stop and delete the canary you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        response = synth.get_canary(Name=canary_name)\n",
    "        status = response['Canary']['Status']['State']    \n",
    "        print('Canary status: {}'.format(status))\n",
    "        if status == 'ERROR':\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \n",
    "        elif status == 'STOPPED':\n",
    "            synth.delete_canary(Name=canary_name)\n",
    "        elif status == 'RUNNING':\n",
    "            synth.stop_canary(Name=canary_name)\n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print('Canary succesfully deleted.')\n",
    "            break\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will delete the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch.delete_alarms(AlarmNames=[canary_alarm_name])\n",
    "print('Alarm deleted')\n",
    "\n",
    "cloudwatch.delete_dashboards(DashboardNames=[dashboard_name])\n",
    "print('Dashboard deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally delete the stack you created for the AWS CodePipeline and Notebook and your done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
